name: Scrape Shutdowns Information

on:
  schedule:
    - cron: '*/30 * * * *'  # Every 30 minutes
  workflow_dispatch:        # Allow manual trigger

concurrency:
  group: cloudflare-pages
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Resolve Playwright version
        id: pw
        shell: bash
        run: bash scripts/resolve-playwright-version.sh

      - name: Cache Playwright browsers
        id: pw-cache
        uses: actions/cache/restore@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-ms-playwright-${{ steps.pw.outputs.version }}

      - name: Cache npm
        id: npm-cache
        uses: actions/cache/restore@v4
        with:
          path: ~/.npm
          key: ${{ runner.os }}-npm-node20-playwright-${{ steps.pw.outputs.version }}
          restore-keys: |
            ${{ runner.os }}-npm-node20-
          
      - name: Install Playwright
        run: |
          npm install playwright@${{ steps.pw.outputs.version }}
          npx playwright install --with-deps chromium

      - name: Read currently deployed stamp (safe)
        id: deployed
        shell: bash
        env:
          CF_PAGES_PROJECT: ${{ secrets.CLOUDFLARE_PAGES_PROJECT }}
          CF_PAGES_URL: ${{ secrets.CLOUDFLARE_PAGES_URL }}
        run: |
          set -euo pipefail
          : "${CF_PAGES_PROJECT:?CLOUDFLARE_PAGES_PROJECT secret is required}"
          BASE_URL="${CF_PAGES_URL:-https://${CF_PAGES_PROJECT}.pages.dev/}"
          [[ "$BASE_URL" != */ ]] && BASE_URL="$BASE_URL/"
          export JSON_URL="${BASE_URL}schedule.json"
          bash scripts/read-deployed-update-stamp.sh

      - name: Save npm cache
        if: steps.npm-cache.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: ~/.npm
          key: ${{ runner.os }}-npm-node20-playwright-${{ steps.pw.outputs.version }}

      - name: Save Playwright browsers
        if: steps.pw-cache.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-ms-playwright-${{ steps.pw.outputs.version }}
          
      - name: Run scraper
        env:
          TARGET_URL: ${{ secrets.TARGET_URL }}
          DATA_VARIABLE_NAME: ${{ secrets.DATA_VARIABLE_NAME }}
        run: node scraper.js

      - name: Scrape summary (redacted)
        shell: bash
        run: |
          set -euo pipefail
          node scripts/print-scrape-summary.js
          echo "Previously deployed update stamp: ${{ steps.deployed.outputs.update }}"

      - name: Set no-cache headers (Cloudflare Pages)
        shell: bash
        run: |
          set -euo pipefail
          cat > scraped-data/_headers <<'EOF'
          /schedule.json
            Cache-Control: no-store, max-age=0
          /latest-metadata.json
            Cache-Control: no-store, max-age=0
          EOF
          
      - name: Deploy to Cloudflare Pages
        shell: bash
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          CF_PAGES_PROJECT: ${{ secrets.CLOUDFLARE_PAGES_PROJECT }}
        run: |
          set -euo pipefail
          : "${CF_PAGES_PROJECT:?CLOUDFLARE_PAGES_PROJECT secret is required}"
          npx --yes wrangler@3 pages deploy ./scraped-data --project-name "$CF_PAGES_PROJECT" --branch main

      - name: Verify Cloudflare Pages updated (bullet-proof)
        shell: bash
        env:
          CF_PAGES_PROJECT: ${{ secrets.CLOUDFLARE_PAGES_PROJECT }}
          CF_PAGES_URL: ${{ secrets.CLOUDFLARE_PAGES_URL }}
        run: |
          set -euo pipefail
          : "${CF_PAGES_PROJECT:?CLOUDFLARE_PAGES_PROJECT secret is required}"
          BASE_URL="${CF_PAGES_URL:-https://${CF_PAGES_PROJECT}.pages.dev/}"
          [[ "$BASE_URL" != */ ]] && BASE_URL="$BASE_URL/"
          export TARGET_JSON_URL="${BASE_URL}schedule.json"
          bash scripts/verify-pages-updated.sh
