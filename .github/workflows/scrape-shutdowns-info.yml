name: Scrape Shutdowns Information

on:
  schedule:
    - cron: '*/30 * * * *'  # Every 30 minutes
  workflow_dispatch:        # Allow manual trigger

concurrency:
  group: hosting-deploy
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pages: write
      id-token: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Resolve Playwright version
        id: pw
        shell: bash
        run: bash scripts/resolve-playwright-version.sh

      - name: Cache Playwright browsers
        id: pw-cache
        uses: actions/cache/restore@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-ms-playwright-${{ steps.pw.outputs.version }}

      - name: Cache npm
        id: npm-cache
        uses: actions/cache/restore@v4
        with:
          path: ~/.npm
          key: ${{ runner.os }}-npm-node20-playwright-${{ steps.pw.outputs.version }}
          restore-keys: |
            ${{ runner.os }}-npm-node20-
          
      - name: Install Playwright
        run: |
          npm install playwright@${{ steps.pw.outputs.version }}
          npx playwright install --with-deps chromium

      - name: Save npm cache
        if: steps.npm-cache.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: ~/.npm
          key: ${{ runner.os }}-npm-node20-playwright-${{ steps.pw.outputs.version }}

      - name: Save Playwright browsers
        if: steps.pw-cache.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-ms-playwright-${{ steps.pw.outputs.version }}
          
      - name: Run scraper
        env:
          TARGET_URL: ${{ secrets.TARGET_URL }}
          DATA_VARIABLE_NAME: ${{ secrets.DATA_VARIABLE_NAME }}
        run: node scraper.js

      - name: Scrape summary (redacted)
        shell: bash
        run: |
          set -euo pipefail
          node scripts/print-scrape-summary.js
          echo "Deployed stamp is verified after deploy."

      - name: Set no-cache headers (Cloudflare Pages)
        shell: bash
        run: |
          set -euo pipefail
          cat > scraped-data/_headers <<'EOF'
          /schedule.json
            Cache-Control: no-store, max-age=0
          /latest-metadata.json
            Cache-Control: no-store, max-age=0
          EOF

      - name: Upload artifact (GitHub Pages)
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./scraped-data

      - name: Deploy to GitHub Pages
        id: gh_deploy
        uses: actions/deploy-pages@v4
          
      - name: Deploy to Cloudflare Pages
        shell: bash
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          CF_PAGES_PROJECT: ${{ secrets.CLOUDFLARE_PAGES_PROJECT }}
        run: |
          set -euo pipefail
          : "${CF_PAGES_PROJECT:?CLOUDFLARE_PAGES_PROJECT secret is required}"
          npx --yes wrangler@3 pages deploy ./scraped-data --project-name "$CF_PAGES_PROJECT" --branch main

      - name: Verify hosting updated (either GitHub Pages or Cloudflare Pages)
        shell: bash
        env:
          GH_PAGE_URL: ${{ steps.gh_deploy.outputs.page_url }}
          CF_PAGES_PROJECT: ${{ secrets.CLOUDFLARE_PAGES_PROJECT }}
          CF_PAGES_URL: ${{ secrets.CLOUDFLARE_PAGES_URL }}
        run: |
          set -euo pipefail

          REPORT_PATH="$RUNNER_TEMP/deploy-report.json"
          export REPORT_PATH

          # GitHub Pages URL
          : "${GH_PAGE_URL:?GitHub Pages deployment URL is required}"
          GH_BASE="$GH_PAGE_URL"
          [[ "$GH_BASE" != */ ]] && GH_BASE="$GH_BASE/"
          GH_JSON_URL="${GH_BASE}schedule.json"

          # Cloudflare Pages URL
          : "${CF_PAGES_PROJECT:?CLOUDFLARE_PAGES_PROJECT secret is required}"
          BASE_URL="${CF_PAGES_URL:-https://${CF_PAGES_PROJECT}.pages.dev/}"
          [[ "$BASE_URL" != */ ]] && BASE_URL="$BASE_URL/"
          CF_JSON_URL="${BASE_URL}schedule.json"

          # Only 3 checks per host; succeed if at least one host updates.
          # Cloudflare Pages is listed first to prioritize it in webhook notifications.
          export MAX_ATTEMPTS=3
          export SLEEP_SECONDS=5
          export TARGET_JSON_URLS="$CF_JSON_URL $GH_JSON_URL"
          bash scripts/verify-any-host-updated.sh

      - name: Upload deploy report artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: deploy-report
          path: ${{ runner.temp }}/deploy-report.json
          retention-days: 3
